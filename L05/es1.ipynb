{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7nyWY1p5NUQTKep3OAo7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andry2327/Machine-Learning-and-Deep-Learning-LAB--Google-Colab---PoliTo/blob/main/L05/es1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUd6CJ0EhgPA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: define AlexNet\n"
      ],
      "metadata": {
        "id": "8cERPY3ghwFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlexNet(torch.nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(AlexNet, self).__init__()\n",
        "    self.layer1 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "        torch.nn.BatchNorm2d(96),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.layer2 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "        torch.nn.BatchNorm2d(256),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.layer3 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.BatchNorm2d(384),\n",
        "        torch.nn.ReLU())\n",
        "    self.layer4 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.BatchNorm2d(384),\n",
        "        torch.nn.ReLU())\n",
        "    self.layer5 = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "        torch.nn.BatchNorm2d(256),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.fc = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(9216, 4096),\n",
        "        torch.nn.ReLU())\n",
        "    self.fc1 = torch.nn.Sequential(\n",
        "        torch.nn.Dropout(0.5),\n",
        "        torch.nn.Linear(4096, 4096),\n",
        "        torch.nn.ReLU())\n",
        "    self.fc2= torch.nn.Sequential(\n",
        "        torch.nn.Linear(4096, num_classes))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.layer4(out)\n",
        "    out = self.layer5(out)\n",
        "    out = out.reshape(out.size(0), -1)\n",
        "    out = self.fc(out)\n",
        "    out = self.fc1(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "6mqTIeTHhy_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: define the loss function\n"
      ],
      "metadata": {
        "id": "NfJK6Mjeh9NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function():\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  return loss_function"
      ],
      "metadata": {
        "id": "0Qz9fVZdh-Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: define the optimizer"
      ],
      "metadata": {
        "id": "h_NsO6SoiGpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "uDhoFp2kiChm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: train function"
      ],
      "metadata": {
        "id": "No72ibZgiY_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_loader, optimizer, cost_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "    net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        # Load data into GPU\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs) # Forward pass\n",
        "        loss = cost_function(outputs, targets) # Apply the loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update parameters\n",
        "        optimizer.zero_grad() # Reset the optimizer\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "SxkFM1J3iaQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: test function"
      ],
      "metadata": {
        "id": "8zPgthRrjOC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "    net.eval() # Strictly needed if network contains layers which have different behaviours between train and test\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            # Load data into GPU\n",
        "            samples += len(inputs)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "S07sY_bXjOY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: loading the data"
      ],
      "metadata": {
        "id": "nfCn_H04j0hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "    # Prepare data transformations and then combine them sequentially\n",
        "    transform = list()\n",
        "    transform.append(T.Resize((227,227)))\n",
        "    transform.append(T.ToTensor()) # Converts Numpy to Pytorch Tensor\n",
        "    transform.append(T.Normalize(mean=[0.5], std=[0.5])) # Normalizes the Tensors between [-1, 1]\n",
        "    transform = T.Compose(transform) # Composes the above transformations into one.\n",
        "    # Load data\n",
        "    full_training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
        "    test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True)\n",
        "    # Create train and validation splits\n",
        "    num_samples = len(full_training_data)\n",
        "    training_samples = int(num_samples*0.8+1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples,\n",
        "    validation_samples])\n",
        "    # Initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "metadata": {
        "id": "n3wW8wAbj1vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "sGiuTZlUj_IQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' Input arguments batch_size: \n",
        "\n",
        "Size of a mini-batch device: GPU where you want to train your network \n",
        "weight_decay: Weight decay co-efficient for regularization of weights \n",
        "momentum: Momentum for SGD optimizer \n",
        "epochs: Number of epochs for training the network \n",
        "\n",
        "'''\n",
        "\n",
        "def main(batch_size=128, device='cuda:0', learning_rate=0.05, weight_decay=0.000001, momentum=0.9, epochs=20):\n",
        "    train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "    # TODO for defining AlexNet\n",
        "    net = AlexNet().to(device)\n",
        "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "    loss_function = get_loss_function()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        train_loss, train_accuracy = train(net, train_loader, optimizer, loss_function)\n",
        "        val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "\n",
        "        print('Epoch: {:d}'.format(e+1))\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "        print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "        print('-----------------------------------------------------')\n",
        "        print('After training:')\n",
        "\n",
        "        train_loss, train_accuracy = test(net, train_loader, loss_function)\n",
        "        val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "        test_loss, test_accuracy = test(net, test_loader, loss_function)\n",
        "\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "        print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "        print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "        print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "bFBUj42OkAFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMAjg9fMkTRo",
        "outputId": "87553fb1-3023-4b0e-c9e8-f6c12261b869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1\n",
            "\t Training loss 0.01289, Training accuracy 39.16\n",
            "\t Validation loss 0.00000, Validation accuracy 46.43\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 46.71\n",
            "\t Validation loss 0.00000, Validation accuracy 46.43\n",
            "\t Test loss 0.00000, Test accuracy 47.38\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.01010, Training accuracy 52.94\n",
            "\t Validation loss 0.00000, Validation accuracy 57.60\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 58.19\n",
            "\t Validation loss 0.00000, Validation accuracy 57.60\n",
            "\t Test loss 0.00000, Test accuracy 57.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00832, Training accuracy 62.34\n",
            "\t Validation loss 0.00000, Validation accuracy 63.58\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 64.53\n",
            "\t Validation loss 0.00000, Validation accuracy 63.58\n",
            "\t Test loss 0.00000, Test accuracy 62.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00717, Training accuracy 67.71\n",
            "\t Validation loss 0.00000, Validation accuracy 68.61\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 71.53\n",
            "\t Validation loss 0.00000, Validation accuracy 68.61\n",
            "\t Test loss 0.00000, Test accuracy 68.52\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00618, Training accuracy 72.26\n",
            "\t Validation loss 0.00000, Validation accuracy 71.25\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 73.95\n",
            "\t Validation loss 0.00000, Validation accuracy 71.25\n",
            "\t Test loss 0.00000, Test accuracy 70.99\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00549, Training accuracy 75.54\n",
            "\t Validation loss 0.00000, Validation accuracy 72.00\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 75.85\n",
            "\t Validation loss 0.00000, Validation accuracy 72.00\n",
            "\t Test loss 0.00000, Test accuracy 71.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00489, Training accuracy 78.23\n",
            "\t Validation loss 0.00000, Validation accuracy 77.60\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 82.18\n",
            "\t Validation loss 0.00000, Validation accuracy 77.60\n",
            "\t Test loss 0.00000, Test accuracy 76.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00431, Training accuracy 80.75\n",
            "\t Validation loss 0.00000, Validation accuracy 77.48\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 83.31\n",
            "\t Validation loss 0.00000, Validation accuracy 77.48\n",
            "\t Test loss 0.00000, Test accuracy 76.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00383, Training accuracy 82.85\n",
            "\t Validation loss 0.00000, Validation accuracy 78.75\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 85.37\n",
            "\t Validation loss 0.00000, Validation accuracy 78.75\n",
            "\t Test loss 0.00000, Test accuracy 78.65\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00344, Training accuracy 84.51\n",
            "\t Validation loss 0.00000, Validation accuracy 81.29\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 89.83\n",
            "\t Validation loss 0.00000, Validation accuracy 81.29\n",
            "\t Test loss 0.00000, Test accuracy 81.17\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00307, Training accuracy 86.30\n",
            "\t Validation loss 0.00000, Validation accuracy 80.60\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 89.26\n",
            "\t Validation loss 0.00000, Validation accuracy 80.60\n",
            "\t Test loss 0.00000, Test accuracy 80.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00273, Training accuracy 87.67\n",
            "\t Validation loss 0.00000, Validation accuracy 82.57\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 92.58\n",
            "\t Validation loss 0.00000, Validation accuracy 82.57\n",
            "\t Test loss 0.00000, Test accuracy 81.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00243, Training accuracy 88.90\n",
            "\t Validation loss 0.00000, Validation accuracy 82.76\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 93.80\n",
            "\t Validation loss 0.00000, Validation accuracy 82.76\n",
            "\t Test loss 0.00000, Test accuracy 82.84\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00217, Training accuracy 90.16\n",
            "\t Validation loss 0.00000, Validation accuracy 81.36\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 92.05\n",
            "\t Validation loss 0.00000, Validation accuracy 81.36\n",
            "\t Test loss 0.00000, Test accuracy 80.34\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00195, Training accuracy 91.13\n",
            "\t Validation loss 0.00000, Validation accuracy 83.21\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 95.63\n",
            "\t Validation loss 0.00000, Validation accuracy 83.21\n",
            "\t Test loss 0.00000, Test accuracy 82.77\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00173, Training accuracy 92.23\n",
            "\t Validation loss 0.00000, Validation accuracy 84.29\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 96.81\n",
            "\t Validation loss 0.00000, Validation accuracy 84.29\n",
            "\t Test loss 0.00000, Test accuracy 83.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00159, Training accuracy 92.95\n",
            "\t Validation loss 0.00000, Validation accuracy 84.36\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 97.31\n",
            "\t Validation loss 0.00000, Validation accuracy 84.36\n",
            "\t Test loss 0.00000, Test accuracy 83.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00139, Training accuracy 93.87\n",
            "\t Validation loss 0.00000, Validation accuracy 83.71\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 97.21\n",
            "\t Validation loss 0.00000, Validation accuracy 83.71\n",
            "\t Test loss 0.00000, Test accuracy 82.68\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00124, Training accuracy 94.46\n",
            "\t Validation loss 0.00000, Validation accuracy 85.19\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 98.12\n",
            "\t Validation loss 0.00000, Validation accuracy 85.19\n",
            "\t Test loss 0.00000, Test accuracy 84.14\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00116, Training accuracy 94.76\n",
            "\t Validation loss 0.00000, Validation accuracy 84.63\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 98.34\n",
            "\t Validation loss 0.00000, Validation accuracy 84.63\n",
            "\t Test loss 0.00000, Test accuracy 83.93\n",
            "-----------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}