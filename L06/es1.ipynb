{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andry2327/Machine-Learning-and-Deep-Learning-LAB--Google-Colab---PoliTo/blob/main/L06/es1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFDZvr0RTjhH"
      },
      "source": [
        "# ResNet\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "rqBByVYpTWuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as T\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "kB0E6LVGYCJo"
      },
      "outputs": [],
      "source": [
        "is_pretrained=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "lLlzUHqvYEyD"
      },
      "outputs": [],
      "source": [
        "ResNet = resnet18(weights='DEFAULT', ) # pretrained=is_pretrained is \n",
        "# Replace the final fully connected layer\n",
        "ResNet.fc = nn.Linear(512, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAccjp57YW7r"
      },
      "source": [
        "Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "R9k5yC-AYXtr"
      },
      "outputs": [],
      "source": [
        "def get_data(batch_size, test_batch_size=256):\n",
        "    # Prepare data transformations and then combine them sequentially\n",
        "    transform = list()\n",
        "    transform.append(T.Resize((227,227)))\n",
        "    transform.append(T.ToTensor()) # Converts Numpy to Pytorch Tensor\n",
        "    transform.append(T.Normalize(mean=[0.5], std=[0.5])) # Normalizes the Tensors between [-1, 1]\n",
        "    transform = T.Compose(transform) # Composes the above transformations into one.\n",
        "    # Load data\n",
        "    full_training_data = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True)\n",
        "    test_data = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True)\n",
        "    # Create train and validation splits\n",
        "    num_samples = len(full_training_data)\n",
        "    training_samples = int(num_samples*0.8+1)\n",
        "    validation_samples = num_samples - training_samples\n",
        "    training_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples,\n",
        "    validation_samples])\n",
        "    # Initialize dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(training_data, batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(validation_data, test_batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, test_batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjvj3gsBZGfI",
        "outputId": "8ddf7e01-f254-4524-9750-556508afe9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "train_set, val_set, test_set = get_data(128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "at8xXx3QamQQ",
        "outputId": "edd8c981-e0af-47df-b64a-0e78ecdbd6f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "# Set the model to evaluation mode\n",
        "ResNet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "GgRARxlFap6S"
      },
      "outputs": [],
      "source": [
        "# Define the classes for CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4wbkzV8a3P-"
      },
      "source": [
        "### Testing: pre_trained=TRUE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "4g9TgwpXatT3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c49fefd-447f-4a37-d0f5-d8ac5c0b29c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE 1/40 --- 2.5%\n",
            "DONE 2/40 --- 5.0%\n",
            "DONE 3/40 --- 7.5%\n",
            "DONE 4/40 --- 10.0%\n",
            "DONE 5/40 --- 12.5%\n",
            "DONE 6/40 --- 15.0%\n",
            "DONE 7/40 --- 17.5%\n",
            "DONE 8/40 --- 20.0%\n",
            "DONE 9/40 --- 22.5%\n",
            "DONE 10/40 --- 25.0%\n",
            "DONE 11/40 --- 27.5%\n",
            "DONE 12/40 --- 30.0%\n",
            "DONE 13/40 --- 32.5%\n",
            "DONE 14/40 --- 35.0%\n",
            "DONE 15/40 --- 37.5%\n",
            "DONE 16/40 --- 40.0%\n",
            "DONE 17/40 --- 42.5%\n",
            "DONE 18/40 --- 45.0%\n",
            "DONE 19/40 --- 47.5%\n",
            "DONE 20/40 --- 50.0%\n",
            "DONE 21/40 --- 52.5%\n",
            "DONE 22/40 --- 55.0%\n",
            "DONE 23/40 --- 57.5%\n",
            "DONE 24/40 --- 60.0%\n",
            "DONE 25/40 --- 62.5%\n",
            "DONE 26/40 --- 65.0%\n",
            "DONE 27/40 --- 67.5%\n",
            "DONE 28/40 --- 70.0%\n",
            "DONE 29/40 --- 72.5%\n",
            "DONE 30/40 --- 75.0%\n",
            "DONE 31/40 --- 77.5%\n",
            "DONE 32/40 --- 80.0%\n",
            "DONE 33/40 --- 82.5%\n",
            "DONE 34/40 --- 85.0%\n",
            "DONE 35/40 --- 87.5%\n",
            "DONE 36/40 --- 90.0%\n",
            "DONE 37/40 --- 92.5%\n",
            "DONE 38/40 --- 95.0%\n",
            "DONE 39/40 --- 97.5%\n",
            "DONE 40/40 --- 100.0%\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  ResNet.cuda()\n",
        "  ResNet.to('cuda:0')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "idx = 0\n",
        "\n",
        "for data in test_set:\n",
        "\n",
        "  imgs, labels = data\n",
        "  if torch.cuda.is_available():\n",
        "    imgs = imgs.cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "  # predict ResNet output\n",
        "  outputs = ResNet(imgs)\n",
        "  _, predicted = torch.max(outputs, 1)\n",
        "  total += labels.size(0)\n",
        "  correct += (predicted == labels).sum().item()\n",
        "\n",
        "  idx+=1\n",
        "  print(f'DONE {idx}/{len(test_set)} --- {round(100*idx/len(test_set), 4)}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "\n",
        "# Print the accuracy\n",
        "print('Accuracy of the ResNet-18 model on the CIFAR10 test set (DEFAULT WEIGTHS): %.2f%%' % accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-6nUm0yhUC5",
        "outputId": "626d0c1f-a586-4a2d-9a5a-7482bb2c6c16"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the ResNet-18 model on the CIFAR10 test set (DEFAULT WEIGTHS): 8.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing: pre_trained=False"
      ],
      "metadata": {
        "id": "xScZdFEzl05Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_function():\n",
        "  loss_function = torch.nn.CrossEntropyLoss()\n",
        "  return loss_function"
      ],
      "metadata": {
        "id": "q-ti2VzZm7gz"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "    optimizer = torch.optim.SGD(params=net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "    return optimizer"
      ],
      "metadata": {
        "id": "Dg6pQgeVm8-y"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_loader, optimizer, cost_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "    net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "        # Load data into GPU\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = net(inputs) # Forward pass\n",
        "        loss = cost_function(outputs, targets) # Apply the loss\n",
        "        loss.backward() # Backward pass\n",
        "        optimizer.step() # Update parameters\n",
        "        optimizer.zero_grad() # Reset the optimizer\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "JzKRIvHFmvgf"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, data_loader, cost_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "    net.eval() # Strictly needed if network contains layers which have different behaviours between train and test\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "            # Load data into GPU\n",
        "            samples += len(inputs)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            # Forward pass\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "z-f2_3ltnKUC"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, device='cuda:0', learning_rate=0.05, weight_decay=0.000001, momentum=0.9, epochs=20):\n",
        "    train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "    # TODO for defining AlexNet\n",
        "    net = ResNet.to(device)\n",
        "    optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "    loss_function = get_loss_function()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        train_loss, train_accuracy = train(net, train_loader, optimizer, loss_function)\n",
        "        val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "\n",
        "        print('Epoch: {:d}'.format(e+1))\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "        print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "        print('-----------------------------------------------------')\n",
        "        print('After training:')\n",
        "\n",
        "        train_loss, train_accuracy = test(net, train_loader, loss_function)\n",
        "        val_loss, val_accuracy = test(net, val_loader, loss_function)\n",
        "        test_loss, test_accuracy = test(net, test_loader, loss_function)\n",
        "\n",
        "        print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "        print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "        print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "        print('-----------------------------------------------------')"
      ],
      "metadata": {
        "id": "Jlso_35ql7GK"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BiegZTuncUJ",
        "outputId": "05f2bf17-7ef8-431e-9b0e-89c9495a1092"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch: 1\n",
            "\t Training loss 0.00517, Training accuracy 78.84\n",
            "\t Validation loss 0.00000, Validation accuracy 81.17\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 83.26\n",
            "\t Validation loss 0.00000, Validation accuracy 81.17\n",
            "\t Test loss 0.00000, Test accuracy 80.58\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00214, Training accuracy 90.93\n",
            "\t Validation loss 0.00000, Validation accuracy 87.00\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 91.19\n",
            "\t Validation loss 0.00000, Validation accuracy 87.00\n",
            "\t Test loss 0.00000, Test accuracy 86.41\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00131, Training accuracy 94.23\n",
            "\t Validation loss 0.00000, Validation accuracy 90.08\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 95.46\n",
            "\t Validation loss 0.00000, Validation accuracy 90.08\n",
            "\t Test loss 0.00000, Test accuracy 89.63\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00087, Training accuracy 96.16\n",
            "\t Validation loss 0.00000, Validation accuracy 90.41\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 96.70\n",
            "\t Validation loss 0.00000, Validation accuracy 90.41\n",
            "\t Test loss 0.00000, Test accuracy 89.92\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00063, Training accuracy 97.21\n",
            "\t Validation loss 0.00000, Validation accuracy 89.79\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 96.41\n",
            "\t Validation loss 0.00000, Validation accuracy 89.79\n",
            "\t Test loss 0.00000, Test accuracy 88.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00052, Training accuracy 97.71\n",
            "\t Validation loss 0.00000, Validation accuracy 90.78\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 97.32\n",
            "\t Validation loss 0.00000, Validation accuracy 90.78\n",
            "\t Test loss 0.00000, Test accuracy 90.08\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00037, Training accuracy 98.37\n",
            "\t Validation loss 0.00000, Validation accuracy 91.20\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 98.21\n",
            "\t Validation loss 0.00000, Validation accuracy 91.20\n",
            "\t Test loss 0.00000, Test accuracy 90.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00035, Training accuracy 98.44\n",
            "\t Validation loss 0.00000, Validation accuracy 91.78\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 98.61\n",
            "\t Validation loss 0.00000, Validation accuracy 91.78\n",
            "\t Test loss 0.00000, Test accuracy 90.97\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00027, Training accuracy 98.85\n",
            "\t Validation loss 0.00000, Validation accuracy 92.13\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.31\n",
            "\t Validation loss 0.00000, Validation accuracy 92.13\n",
            "\t Test loss 0.00000, Test accuracy 91.98\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00017, Training accuracy 99.27\n",
            "\t Validation loss 0.00000, Validation accuracy 91.56\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.03\n",
            "\t Validation loss 0.00000, Validation accuracy 91.56\n",
            "\t Test loss 0.00000, Test accuracy 91.06\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00021, Training accuracy 99.08\n",
            "\t Validation loss 0.00000, Validation accuracy 92.24\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.22\n",
            "\t Validation loss 0.00000, Validation accuracy 92.24\n",
            "\t Test loss 0.00000, Test accuracy 91.18\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00017, Training accuracy 99.28\n",
            "\t Validation loss 0.00000, Validation accuracy 92.92\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.56\n",
            "\t Validation loss 0.00000, Validation accuracy 92.92\n",
            "\t Test loss 0.00000, Test accuracy 92.35\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00009, Training accuracy 99.61\n",
            "\t Validation loss 0.00000, Validation accuracy 92.50\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.45\n",
            "\t Validation loss 0.00000, Validation accuracy 92.50\n",
            "\t Test loss 0.00000, Test accuracy 91.70\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00011, Training accuracy 99.50\n",
            "\t Validation loss 0.00000, Validation accuracy 92.32\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.56\n",
            "\t Validation loss 0.00000, Validation accuracy 92.32\n",
            "\t Test loss 0.00000, Test accuracy 92.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00005, Training accuracy 99.83\n",
            "\t Validation loss 0.00000, Validation accuracy 93.60\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.91\n",
            "\t Validation loss 0.00000, Validation accuracy 93.60\n",
            "\t Test loss 0.00000, Test accuracy 92.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00008, Training accuracy 99.66\n",
            "\t Validation loss 0.00000, Validation accuracy 92.21\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.53\n",
            "\t Validation loss 0.00000, Validation accuracy 92.21\n",
            "\t Test loss 0.00000, Test accuracy 91.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00006, Training accuracy 99.71\n",
            "\t Validation loss 0.00000, Validation accuracy 92.38\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.68\n",
            "\t Validation loss 0.00000, Validation accuracy 92.38\n",
            "\t Test loss 0.00000, Test accuracy 92.15\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00010, Training accuracy 99.54\n",
            "\t Validation loss 0.00000, Validation accuracy 92.60\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.40\n",
            "\t Validation loss 0.00000, Validation accuracy 92.60\n",
            "\t Test loss 0.00000, Test accuracy 91.64\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00007, Training accuracy 99.70\n",
            "\t Validation loss 0.00000, Validation accuracy 92.69\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.73\n",
            "\t Validation loss 0.00000, Validation accuracy 92.69\n",
            "\t Test loss 0.00000, Test accuracy 92.45\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00008, Training accuracy 99.67\n",
            "\t Validation loss 0.00000, Validation accuracy 92.64\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00000, Training accuracy 99.52\n",
            "\t Validation loss 0.00000, Validation accuracy 92.64\n",
            "\t Test loss 0.00000, Test accuracy 92.01\n",
            "-----------------------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPqmiyI9ZLbbG9YyCPRlwC",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}